{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s69X9bcxxfsN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Mon_May__3_19:15:13_PDT_2021\n",
      "Cuda compilation tools, release 11.3, V11.3.109\n",
      "Build cuda_11.3.r11.3/compiler.29920130_0\n",
      "Thu Jun 13 15:26:43 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.11    Driver Version: 525.60.11    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 31%   52C    P8    17W / 320W |   9882MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 90%   87C    P2   292W / 320W |   8698MiB / 10240MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 93%   88C    P2   284W / 320W |   8698MiB / 10240MiB |     72%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 81%   89C    P2   262W / 320W |   8696MiB / 10240MiB |     75%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "|100%   88C    P2   274W / 320W |   8696MiB / 10240MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 92%   85C    P2   246W / 320W |   4330MiB / 10240MiB |     46%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 43%   87C    P2   202W / 320W |   3006MiB / 10240MiB |     45%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 43%   87C    P2   167W / 320W |   4376MiB / 10240MiB |     43%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11998      C   python                           2470MiB |\n",
      "|    0   N/A  N/A     45792      C   python                           2470MiB |\n",
      "|    0   N/A  N/A     46151      C   python                           2470MiB |\n",
      "|    0   N/A  N/A     46506      C   python                           2470MiB |\n",
      "|    1   N/A  N/A     45792      C   python                           8696MiB |\n",
      "|    2   N/A  N/A     46151      C   python                           8696MiB |\n",
      "|    3   N/A  N/A     46506      C   python                           8694MiB |\n",
      "|    4   N/A  N/A     11998      C   python                           8694MiB |\n",
      "|    5   N/A  N/A     19067      C   python                           4328MiB |\n",
      "|    6   N/A  N/A     19067      C   python                           3004MiB |\n",
      "|    7   N/A  N/A     19067      C   python                           4374MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkgJgEjBxfsX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.7.1+cu110\n",
      "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 GB\u001b[0m \u001b[31m254.3 kB/s\u001b[0m eta \u001b[36m1:15:29\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install necessary packages (Kaggle-> CUDA Version: 11.0 | Colab-> CUDA Version: 11.2).\n",
    "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install pyyaml==5.1\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu110/torch1.7/index.html\n",
    "!pip install wandb --upgrade     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UycmKEW3xfsZ",
    "outputId": "0adc376e-17e0-4045-bf13-ae168d340e8a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'termcolor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4072/3345748390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check installed packages:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtermcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1.7.1+cu110'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf' Torch version is: {torch.__version__}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'termcolor'"
     ]
    }
   ],
   "source": [
    "# Check installed packages:\n",
    "from termcolor import colored\n",
    "\n",
    "import torch\n",
    "assert torch.__version__ == '1.7.1+cu110', f' Torch version is: {torch.__version__}'\n",
    "print(colored(f' Torch version is: {torch.__version__}', 'green', attrs=['bold'])) \n",
    "\n",
    "import torchvision\n",
    "assert torchvision.__version__ == '0.8.2+cu110', f'Torchvision version is: {torchvision.__version__}' \n",
    "print(colored(f' Torchvision version is: {torchvision.__version__}', 'green', attrs=['bold'])) \n",
    "\n",
    "import detectron2\n",
    "assert detectron2.__version__ == '0.5', f' Detectron2 version is: {detectron2.__version__}' \n",
    "print(colored(f' Detectron2 version is: {detectron2.__version__}', 'green', attrs=['bold'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sh2cGCpIxfsa",
    "outputId": "9a7b710f-cc57-41df-acaf-6892296cc494"
   },
   "outputs": [],
   "source": [
    "# Check GPU properties:\n",
    "if torch.cuda.is_available():\n",
    "    gpu = !nvidia-smi -L \n",
    "    gpu_name_and_id = ''.join(gpu[0].split(':')[1:])\n",
    "    print(colored('This notebook uses a GPU' + gpu_name_and_id, 'green', attrs=['bold']))\n",
    "else:\n",
    "    print(colored('No GPU available','red', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyAvNCJMmvFF",
    "outputId": "e2236246-a82f-4ac2-d12e-1a68ea11d305"
   },
   "outputs": [],
   "source": [
    "# Import necessary detectron2 utilities:\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor, default_argument_parser, default_setup, hooks, launch\n",
    "from detectron2.engine.hooks import HookBase\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config.config import CfgNode as CN\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode \n",
    "from detectron2.utils.logger import setup_logger, log_every_n_seconds, logging \n",
    "import detectron2.utils.comm as comm\n",
    "\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader, DatasetMapper\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data.detection_utils import transform_instance_annotations\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "from detectron2.evaluation import COCOEvaluator, RotatedCOCOEvaluator, inference_on_dataset, inference_context\n",
    "\n",
    "from detectron2.modeling import build_model\n",
    "\n",
    "from detectron2.structures.boxes import BoxMode\n",
    "\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "setup_logger() # Setup detectron2 logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKuwtbaONA5r"
   },
   "outputs": [],
   "source": [
    "# Import some common libraries:\n",
    "import numpy as np\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import random\n",
    "import yaml \n",
    "import os, json, matplotlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqZd5qXrK6Vv"
   },
   "source": [
    "# Load and register Train, Test and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBnI0MkVWbZN",
    "outputId": "22178fc1-fe37-4380-8155-a0fe7aef756a"
   },
   "outputs": [],
   "source": [
    "# If u are using google colab run this:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBpNsv5mEE9P"
   },
   "outputs": [],
   "source": [
    "dataset_dir = '/content/drive/MyDrive/Datasets/HRSID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tsux9ZAY0ajQ"
   },
   "outputs": [],
   "source": [
    "def register_COCO_datasets(dataset_dir):\n",
    "\n",
    "    \"\"\"Loads, registers and names the 3 parts (Train, Test and Validation) of \n",
    "    the dataset from the given directory. Every time that the specific function \n",
    "    is called it creates a unique name for each one of Train, Test and Validation\n",
    "    sets, because detectron2 can't register a previous registered dataset with \n",
    "    the same dataset name.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): Dataset's directory.\n",
    "\n",
    "    Returns:\n",
    "        dataset_names (list): A list of train, test and validation set names.\n",
    "\n",
    "    \"\"\"\n",
    "    _, SubFolders, _ = next(os.walk(dataset_dir))\n",
    "    SubFolders.sort()\n",
    "    \n",
    "    # Subfolder check.\n",
    "    SubFoldrs =[]\n",
    "    for folder_name in SubFolders:\n",
    "        if 'set' in folder_name: SubFoldrs.append(folder_name)     \n",
    "\n",
    "    rint = str(random.randint(0, 1000))\n",
    "    Setnames = ['Training','Validation','Test']\n",
    "    dataset_names = []\n",
    "\n",
    "    for folder_name, setname in zip(SubFoldrs, Setnames):\n",
    "        files_dir = os.path.join(dataset_dir, folder_name, 'data')\n",
    "        json_dir = os.path.join(dataset_dir, folder_name, 'labels.json')\n",
    "        dataset_name = 'HRSID' + '_' + setname + '_' + rint\n",
    "        register_coco_instances(dataset_name, {}, json_dir, files_dir)\n",
    "        dataset_names.append(dataset_name)\n",
    "    \n",
    "    print(colored(f'Datasets: {dataset_names[0]}, {dataset_names[1]}, {dataset_names[2]} Registered successfully!','green', attrs=['bold']))\n",
    "\n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny5KpwQTA4Vz"
   },
   "outputs": [],
   "source": [
    "def extract_sets_id(dataset_names):\n",
    "\n",
    "    \"\"\"Extracts the id of each one of the three sets (Training, Test and \n",
    "    Validation) and uses it as id for accesing the specific dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_names (list): A list of train, test and validation set names.\n",
    "\n",
    "    Returns:\n",
    "        train_id (int): The id of the train set.\n",
    "        train_id (int): The id of the test set.\n",
    "        train_id (int): The id of the validation set.\n",
    "\n",
    "    \"\"\"\n",
    "    train_id, test_id, valid_id = 99,99,99\n",
    "\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        if dataset_name.split('_')[1] == 'Training': \n",
    "            train_id = i\n",
    "        elif dataset_name.split('_')[1] == 'Test':\n",
    "            test_id = i\n",
    "        else:\n",
    "            valid_id = i\n",
    "\n",
    "    return train_id, valid_id, test_id        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zEVXVe8WkzT",
    "outputId": "970261ed-55d2-4fe1-dad2-3de0ece52bc0"
   },
   "outputs": [],
   "source": [
    "dataset_names = register_COCO_datasets(dataset_dir)\n",
    "train_id, val_id, test_id = extract_sets_id(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-Xu2qE6Rtq-",
    "outputId": "f5f007c2-68a0-48e3-c29e-453e3c7a751f"
   },
   "outputs": [],
   "source": [
    "#Get dicts and metadata from train data.\n",
    "dicts_train = detectron2.data.get_detection_dataset_dicts(dataset_names[train_id])\n",
    "metadata_train = MetadataCatalog.get(dataset_names[train_id])\n",
    "metadata_train.thing_colors = [(0, 255, 0)] #255 165 0 \n",
    "\n",
    "#Get dicts and metadata from validation data.\n",
    "dicts_val = detectron2.data.get_detection_dataset_dicts(dataset_names[val_id])\n",
    "metadata_val = MetadataCatalog.get(dataset_names[val_id])\n",
    "metadata_val.thing_colors = [(0, 255, 0)]\n",
    "\n",
    "#Get dicts and metadata from test data.\n",
    "dicts_test = detectron2.data.get_detection_dataset_dicts(dataset_names[test_id])\n",
    "metadata_test = MetadataCatalog.get(dataset_names[test_id])\n",
    "metadata_test.thing_colors = [(0, 255, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEpPBj9qQdsp",
    "outputId": "5cde514d-0c43-4ab3-8d17-46443b6ddd4c"
   },
   "outputs": [],
   "source": [
    "# Check the class names of the training and validation set. \n",
    "assert metadata_test.thing_classes == metadata_train.thing_classes == metadata_val.thing_classes\n",
    "print(f'Class names in Training set: {metadata_train.thing_classes}')\n",
    "print(f'Class names in Validation set: {metadata_val.thing_classes}')\n",
    "print(f'Class names in Test set: {metadata_test.thing_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_wMxlwBXkO0"
   },
   "source": [
    "# **Display some random training images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdexTNot6c-0"
   },
   "outputs": [],
   "source": [
    "def show_rand_img(dicts, metadata, img_id=None):\n",
    "\n",
    "    \"\"\"Selects an image from the given set and displays it with its \n",
    "    annotations.\n",
    "\n",
    "    Args:\n",
    "        dicts (list): List of dataset's dictionaries.\n",
    "        metadata (detectron2.data.catalog.Metadata): Dataset's metadata.\n",
    "        img_id(int): Image id for display.\n",
    "        \n",
    "    \"\"\"\n",
    "    image_dicts = []\n",
    "    \n",
    "    if img_id is not None:\n",
    "        for i,props in enumerate(dicts):\n",
    "            if props['image_id'] == img_id:\n",
    "                image_dicts = dicts[i]\n",
    "            random_image_properties = image_dicts  \n",
    "        if random_image_properties==[]:\n",
    "            print(f'The requested image id it is not contained in the {metadata.name} set...')\n",
    "            print('A random image will be selected for visualization...')\n",
    "\n",
    "            #Select a random image from the given set and extract its properties.\n",
    "            random_im_number = random.randint(0, len(dicts)) \n",
    "            random_image_properties = dicts[random_im_number]\n",
    "            \n",
    "    else:\n",
    "        #Select a random image from the given set and extract its properties.\n",
    "        random_im_number = random.randint(0, len(dicts)) \n",
    "        random_image_properties = dicts[random_im_number]\n",
    "\n",
    "    #Read image path.\n",
    "    path = random_image_properties['file_name']\n",
    "    set_name = metadata.name\n",
    "\n",
    "    #Read image and add propeties.\n",
    "    random_image_4_viz = cv2.imread(path) \n",
    "    \n",
    "    #Print image id.\n",
    "    cv2.putText(\n",
    "        random_image_4_viz, #numpy array on which text is written\n",
    "        'Image ID: ' +  str(random_image_properties['image_id']), #text\n",
    "        (670, 770), #position at which writing has to start\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "        0.4, #font size\n",
    "        (255, 0, 0, 0), #font color BGR\n",
    "        1) #font stroke\n",
    "    \n",
    "    #Print set name.\n",
    "    cv2.putText(random_image_4_viz, set_name, \n",
    "                (670,790), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0, 0), 1) \n",
    "\n",
    "    #Visualize the selected image together with its annotations.\n",
    "    viz = Visualizer(random_image_4_viz, metadata=metadata, scale=1., instance_mode=ColorMode.SEGMENTATION)\n",
    "    out = viz.draw_dataset_dict(random_image_properties)\n",
    "   \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(out.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zYdxuesIxfsk",
    "outputId": "5a7fd7c7-e08a-4700-d619-941f727db9f0"
   },
   "outputs": [],
   "source": [
    "# Show 2 random training images:\n",
    "for i in range(2): show_rand_img(dicts_train, metadata_train, img_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSMvs4Zt0583"
   },
   "source": [
    "# **Gpu memory allocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "spgkPr3JDnq0",
    "outputId": "88bcd82d-d321-4f5b-ff19-d50b8230bbcc"
   },
   "outputs": [],
   "source": [
    "#Gpu memory allocation:\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlqXIXXhW8dA"
   },
   "source": [
    "# **Utils**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_F0xMAcxfsq"
   },
   "outputs": [],
   "source": [
    "def save_config(cfg, file_name):\n",
    "\n",
    "    \"\"\"Saves the given configuration in the output and session's directories.\n",
    "\n",
    "    Args:\n",
    "        cfg (detectron2.config.config.CfgNode): Model's configuration.\n",
    "        file_name (str): Name for saving .yaml file.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create the output directory.\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    saving_path = cfg.OUTPUT_DIR + '/' + file_name\n",
    "    \n",
    "    #Save configuration to file.\n",
    "    with open(saving_path, \"w\") as f:\n",
    "        f.write(cfg.dump())   \n",
    "    \n",
    "    #Copy output configuration in the session's directory.\n",
    "    !cp $saving_path $file_name\n",
    "    \n",
    "    #Check if the given configuration saved successfully.\n",
    "    test_cfg = get_cfg()\n",
    "    test_cfg.merge_from_file(file_name)\n",
    "    if test_cfg:\n",
    "        print(\"Model configuration saved successfully!\")\n",
    "        cfg.freeze()\n",
    "    else: print(\"Error in model configuration saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdsLN60lxfsq"
   },
   "outputs": [],
   "source": [
    "def datetime_stamp(time_zone :str = 'Europe/Athens'):  \n",
    "  \n",
    "    \"\"\"Creates a datetime stamp based on the given timezone.\n",
    "\n",
    "    Args:\n",
    "        time_zone (str): Name of the timezone in which we want the produced \n",
    "        date-time stamp.\n",
    "\n",
    "    Returns: \n",
    "        current_date_and_time (str): The current date and time in \n",
    "        dd_mm_yyyy__hh_mm format, based on the given timezone.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    timezone = pytz.timezone(time_zone)\n",
    "    current_date_and_time = datetime.datetime.now(tz=timezone)\n",
    "    current_date_and_time = current_date_and_time.strftime(\"%d_%m_%Y__%H_%M\")\n",
    "\n",
    "    return current_date_and_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhTLxrvoxfsr"
   },
   "source": [
    "# **Custom hooks for validation loss logging during training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3PL_QBnxfsr"
   },
   "outputs": [],
   "source": [
    "class LossEvalHook(HookBase):\n",
    "\n",
    "    '''Custom hook that calculates and prints validation loss \n",
    "    in every 20 iterations.\n",
    "\n",
    "    This class was copied from: https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        \n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a86m2pWxfst"
   },
   "source": [
    "# **Mappers with and without augmentations for training and inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfYS8tc6xfst"
   },
   "outputs": [],
   "source": [
    "def mapper(dataset_dict):\n",
    "\n",
    "    '''\n",
    "    A function that takes a Detectron2's Dataset (list of dictionaries),\n",
    "    and transforms it into a format that is used by the model.\n",
    "\n",
    "    This mapper is also responsible for augmenting the training data by using a \n",
    "    sequence of photometric and geometric transformations applied on the training \n",
    "    images and their corresponding annotations.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (list): Image properties and annotations of the input dataset.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Make a copy of the input dict.\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)\n",
    "\n",
    "    # Load image.\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\") # Reads the image based on its filename. HWC image | 0-255 | uint8\n",
    "    \n",
    "    # Define a sequence of transformations:\n",
    "    transformation_list = [T.Resize((1400,1400), interp=PIL.Image.BILINEAR),\n",
    "                      T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
    "                      T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "                      T.RandomApply(T.RandomBrightness(0.7, 1.2), prob=0.3), \n",
    "                      T.RandomApply(T.RandomContrast(0.7, 1.2), prob=0.3),\n",
    "                      T.RandomApply(T.RandomSaturation(0.7, 1.2), prob=0.3)\n",
    "                      ]\n",
    "    \n",
    "    # Transform image.transforms.ToTensor()(np.array(img))\n",
    "    auginput = T.AugInput(image) # Define the augmentation input (\"image\" required, others optional). \n",
    "    \n",
    "    # Transform image.\n",
    "    auginput, transforms = T.apply_transform_gens(transformation_list, auginput)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(copy.deepcopy(auginput.image.transpose(2, 0, 1).astype(dtype=np.uint8))) #dtype=np.uint8 'float32'\n",
    "    \n",
    "    # Transform annotations.\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, [transforms], auginput.image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    \n",
    "    instances = utils.annotations_to_instances(annos, auginput.image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tW21J4Zhxfst"
   },
   "outputs": [],
   "source": [
    "def mapper_wo(dataset_dict):\n",
    "    \n",
    "    '''\n",
    "    Mapper without augmentations.\n",
    "    This mapper is used only for inference.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dict (list): Image properties and annotations of the input dataset.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Make a copy of the input dict.\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  \n",
    "    \n",
    "    # Load image.\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\") # Reads the image based on its filename. HWC image | 0-255 | uint8\n",
    "\n",
    "    # Transform image.\n",
    "    auginput = T.AugInput(image) # Define the augmentation input (\"image\" required, others optional). \n",
    "    transform = T.Resize((1400, 1400),interp=PIL.Image.BILINEAR)(auginput) #Apply the transformation.\n",
    "    image = torch.from_numpy(copy.deepcopy(auginput.image.transpose(2, 0, 1)))  #Change channel order from w*h*c to c*w*h\n",
    "\n",
    "    # Transform annotations.\n",
    "    annos = [\n",
    "        transform_instance_annotations(annotation, [transform], image.shape[1:])\n",
    "        for annotation in dataset_dict.pop(\"annotations\")\n",
    "        if annotation.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "\n",
    "    # Convert annotations to Instances and filter out the empty ones. \n",
    "    instances = utils.annotations_to_instances(annos, image.shape[1:])\n",
    "\n",
    "    # Add image and instances to the dictionary.\n",
    "    dataset_dict[\"image\"] = image #image[0,:,:]because utils.read_image() uses pillow converts the image from grayscale to bgr with 3 identical channels.\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRnLPuPUxfsu"
   },
   "outputs": [],
   "source": [
    "def show_original_augmented(dataset_dict, dataset_metadata):\n",
    "\n",
    "    ''' This function selects a random image with its annotations and displays it \n",
    "    side by side with the corresponding augmented image with the new annotations \n",
    "    that are extracted from the mapper used in training.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (list): Image properties and annotations of the input dataset.\n",
    "        dataset_metadata (detectron2.data.Metadata): Metadata of the given set.\n",
    "    '''\n",
    "\n",
    "    # Select a random image.\n",
    "    random_im_number = random.randint(181, len(dataset_dict)) \n",
    "    \n",
    "    # Draw annotations on the original image.\n",
    "    random_image_4_viz = cv2.imread(dataset_dict[random_im_number]['file_name']) \n",
    "    viz = Visualizer(random_image_4_viz, dataset_metadata, scale=1., instance_mode=ColorMode.SEGMENTATION)\n",
    "    out1 = viz.draw_dataset_dict(dataset_dict[random_im_number])\n",
    "    shape_min = random_image_4_viz.shape[0]\n",
    "    number_of_gt_boxes = len(dataset_dict[random_im_number]['annotations'])\n",
    "    \n",
    "    # Draw annotations on the augmented image.\n",
    "    aug_inputs = mapper(dataset_dict[random_im_number])\n",
    "    random_image_4_viz2 = np.moveaxis(np.array(aug_inputs['image']),0,-1)\n",
    "    viz = Visualizer(random_image_4_viz2, dataset_metadata, scale=1., instance_mode=ColorMode.SEGMENTATION)\n",
    "    number_of_new_gt_boxes = len(aug_inputs['instances'])\n",
    "    out2 = viz.overlay_instances(boxes=aug_inputs['instances'].gt_boxes.tensor.to('cpu'),assigned_colors=['magenta']*number_of_new_gt_boxes)\n",
    "    shape_max = random_image_4_viz2.shape[0]\n",
    "    \n",
    "    # The instance number in original and augmented image must be the same.\n",
    "    assert number_of_gt_boxes==number_of_new_gt_boxes\n",
    "    \n",
    "    # Pad the smaller image (original image) with zeros.\n",
    "    if shape_max<shape_min: shape_min, shape_max = shape_max, shape_min\n",
    "    frame_width = int((shape_max - shape_min)/2)\n",
    "    npad = ((frame_width, frame_width), (frame_width, frame_width), (0, 0))\n",
    "    out_collage = np.hstack((np.pad(out1.get_image(), npad),out2.get_image()))\n",
    "    \n",
    "    # Print image id.\n",
    "    cv2.putText(\n",
    "        out_collage, #numpy array on which text is written\n",
    "        'Image ID: '+  str(dataset_dict[random_im_number]['image_id']), #text\n",
    "        (int(2*shape_max-shape_max/5), int(0.97*shape_max)), #position at which writing has to start\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "        1., #font size\n",
    "        (255, 0, 0, 0),\n",
    "        2) #font stroke\n",
    "    \n",
    "    # Display the collage.\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(out_collage);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Tou73uB8xfsu",
    "outputId": "5e3a53cd-aee4-4f9f-c5fe-35704598eed0"
   },
   "outputs": [],
   "source": [
    "# Show 2 random images with their corresponding augmented ones:\n",
    "for i in range(2):show_original_augmented(dicts_train, metadata_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCJcd5sYxfsu"
   },
   "source": [
    "# **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb1Y-Q3wxfsu"
   },
   "outputs": [],
   "source": [
    "assert False #Break the execution here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ohE7O68AM46",
    "outputId": "f675336f-1f0c-4156-c4cd-5ef4d6e64a1b"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.SEED = 42\n",
    "\n",
    "# Define training and validation datasets:\n",
    "#-------------------------------------------------------------------------------\n",
    "cfg.DATASETS.TRAIN = (dataset_names[train_id],) \n",
    "cfg.DATASETS.TEST = (dataset_names[val_id],) # Validation Data!!!\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # Only one class (ship). \n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 0 # -> All weights are trainable.\n",
    "\n",
    "# Input properties:\n",
    "#-------------------------------------------------------------------------------\n",
    "# Because we created a custom mapper this is not used...\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (1400,)\n",
    "cfg.INPUT.RANDOM_FLIP = \"none\"\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1400\n",
    "cfg.INPUT.MIN_SIZE_TEST = 1400 #0 = disable resizing in testing\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1400\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Solver properties:\n",
    "#-------------------------------------------------------------------------------\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.MOMENTUM = 0.90\n",
    "cfg.SOLVER.BASE_LR = 0.005 # base learning rate ~0.0005 if u want to continiue training else 0.005 \n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 2000 # Save a checkpoint after every this number of iterations.\n",
    "cfg.SOLVER.MAX_ITER = 4000\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "cfg.SOLVER.WARMUP_FACTOR = 1.0 / 500\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "cfg.SOLVER.WARMUP_METHOD = \"linear\"\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = 'WarmupCosineLR'\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Normalization parameters and input format:\n",
    "#-------------------------------------------------------------------------------\n",
    "cfg.MODEL.PIXEL_MEAN = [17.467]*3 \n",
    "cfg.MODEL.PIXEL_STD = [13.975]*3 \n",
    "cfg.INPUT.FORMAT = \"BGR\"\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Define the corresponding anchor box properties for every Pi level.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "cfg.MODEL.RPN.IN_FEATURES = ['p2','p3','p4','p5','p6'] \n",
    "\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [\n",
    "                                    [16.174, 24.418, 13.39], # P2\n",
    "                                    [25.037, 27.657, 28.812], # P3\n",
    "                                    [43.000, 53.000, 84.000], # P4\n",
    "                                    [102.000, 176.000, 349.000], # P5\n",
    "                                    [249.404, 128.570, 668.358] # P6\n",
    "                                    ] \n",
    "\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [\n",
    "                                            [0.581, 0.272, 1.328], # P2\n",
    "                                            [1.487, 2.997, 0.272], # P3\n",
    "                                            [2.030, 0.726, 2.012], # P4\n",
    "                                            [0.775, 1.269, 0.874], # P5\n",
    "                                            [0.500, 1.000, 2.000] # P6\n",
    "                                            ]\n",
    "\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[0]] # the bounding boxes are 'normal'.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Configuration for the nubmer of ROIs before and after the application of NMS.\n",
    "#-------------------------------------------------------------------------------\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 3000\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 3000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 3000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 3000\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Configuration for validation and testing.\n",
    "#-------------------------------------------------------------------------------\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 95\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "cfg.TEST.EVAL_PERIOD = 250\n",
    "cfg.TEST.AUG.FLIP = False\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Define the directory where output files are written.\n",
    "cfg.OUTPUT_DIR = './' +'_'+ datetime_stamp() \n",
    "\n",
    "# Save configuration\n",
    "config_filename =  'configuration_' + datetime_stamp() + '.yaml'\n",
    "save_config(cfg,config_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXOP47Ukxfsy"
   },
   "source": [
    "# **Custom trainer with custom mappers and hooks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adbvhS77xfs3"
   },
   "outputs": [],
   "source": [
    "class MyTrainer(DefaultTrainer):\n",
    "\n",
    "    '''Trainer that is based on the 'DefaultTrainer' class but with some \n",
    "    extra customizations regarding its default methods.'''\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=mapper) #mapper with augmentations \n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(cfg, dataset_name, mapper=mapper_wo) #mapper without augmentations \n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "             \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                mapper=mapper\n",
    "            )\n",
    "        ))\n",
    "        return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QS2gSrE-xfs4",
    "outputId": "02640ea9-39b9-4bc0-969b-ec1bfc022275"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "personal_wandb_api_key = ''\n",
    "wandb.login(key=personal_wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAac4ejsxfs4"
   },
   "source": [
    "# **Log training data in Weights & Biases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6SmiLHfxfs4"
   },
   "outputs": [],
   "source": [
    "# Train the model (one run without sweeps)\n",
    "wandb.init(mode=\"disabled\") #close wandb session if its running\n",
    "\n",
    "# Start wandb session.\n",
    "project_name = \"FasterRCNN\" \n",
    "entity_name = \"jasonman\"\n",
    "wandb.init(project=project_name, entity=entity_name, sync_tensorboard=True) \n",
    "\n",
    "# Start training.\n",
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# After training we must terminate wandb session.\n",
    "wandb.init(mode=\"disabled\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCHoQWJ6lPQO"
   },
   "outputs": [],
   "source": [
    "# In colab u can see the training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir cfg.OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amFRVvxhxfs4"
   },
   "source": [
    "# **Train the model with sweeps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNBfWYozxfs5"
   },
   "outputs": [],
   "source": [
    "# Define the search method for hyperparameter sweeps.\n",
    "sweep_config = {'method': 'random'}\n",
    "\n",
    "# Define the target metric and add it to the configuration.\n",
    "metric = {'name': 'bbox/AP50', 'goal': 'maximize'}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "# Set the hyperparameters for tuning and the coresponding search range.\n",
    "parameters_dict = {\n",
    "\n",
    "    'momentum': {\n",
    "    'values': [0.9, 0.95, 0.99]},\n",
    "    'anchor_sizes': {\n",
    "    'values': [cfg.MODEL.ANCHOR_GENERATOR.SIZES]},\n",
    "    'anchor_ratios': {\n",
    "    'values': [cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS]},\n",
    "    'IOU_thresholds': {\n",
    "    'values': [[0.2], [0.3], [0.4], [0.5]]}, \n",
    "    'pre_NMS_top_train': {\n",
    "    'values': [1000, 2000, 3000, 4000, 5000]},\n",
    "    'post_NMS_top_train': {\n",
    "    'values': [1000, 2000, 3000]},\n",
    "    'pre_NMS_top_test': {\n",
    "    'values': [1000, 2000, 3000, 4000, 5000]},\n",
    "    'post_NMS_top_test': {\n",
    "    'values': [1000, 2000, 3000]},\n",
    "    'learning_rate':{'value': cfg.SOLVER.BASE_LR}\n",
    "}\n",
    "\n",
    "# Add parameters dictionary to sweep configuration.\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# If u want to add new configuration parameters:\n",
    "parameters_dict.update({'epochs': {'value': 6}})\n",
    "parameters_dict.update({'batch_size': {'values': [2,4]}})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8ON3iAw7Z86",
    "outputId": "580be95f-cc6e-4e6e-c5d5-00d3cce7eb76"
   },
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYItkBa-xfs6"
   },
   "outputs": [],
   "source": [
    "def sweeptrain(cfg):\n",
    "\n",
    "    '''This function changes the model's configuration parameters according to\n",
    "    the extracted parameter from the sweep configuration. Every time that this function\n",
    "    is called, it defines a new model that is trained with a new set of hyperparameters. \n",
    "    \n",
    "    Args:\n",
    "        cfg (detectron2.config.config.CfgNode): Base configuration of the model.\n",
    "    '''\n",
    "    \n",
    "    # Start wandb session.\n",
    "    wandb.init(project=\"FasterRCNN\", entity=\"jasonman\", sync_tensorboard=True)\n",
    "    \n",
    "    number_of_epochs_2_train = 6\n",
    "    number_of_training_imgs = 3461\n",
    "    \n",
    "    # Change configuration according to sweep cofig values.\n",
    "    cfg.defrost()\n",
    "    cfg.SOLVER.IMS_PER_BATCH = wandb.config.batch_size\n",
    "    cfg.SOLVER.BASE_LR  = wandb.config.learning_rate\n",
    "    cfg.SOLVER.MOMENTUM = wandb.config.momentum\n",
    "    cfg.SOLVER.MAX_ITER = round(number_of_epochs_2_train*number_of_training_imgs/wandb.config.batch_size)\n",
    "    cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS = wandb.config.IOU_thresholds\n",
    "    cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = wandb.config.pre_NMS_top_train\n",
    "    cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = wandb.config.post_NMS_top_train\n",
    "  \n",
    "    # Create a new trainer based on the previous configuration parameters.\n",
    "    trainer = MyTrainer(cfg) \n",
    "    trainer.resume_or_load(resume=False)\n",
    "    \n",
    "    # Start training when the function is called..\n",
    "    return trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5FpDRGRxfs9"
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"FasterRCNN\") # run this when u are changing sweep configuration.\n",
    "# sweep_id = 'gj021nte' if u want to continiue experiments using an older swep config.\n",
    "wandb.agent(project=\"FasterRCNN\", sweep_id = sweep_id, function= lambda:sweeptrain(cfg), count=1)\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3SO2s4KlYxr"
   },
   "outputs": [],
   "source": [
    "# In colab u can see the training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDPGJAnbxfs-"
   },
   "source": [
    "# **Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZcvTyB6xfs-"
   },
   "outputs": [],
   "source": [
    "# experiment_folder = cfg.OUTPUT_DIR \n",
    "experiment_folder = '/content'\n",
    "\n",
    "def load_json_arr(json_path):\n",
    "\n",
    "    '''Auxiliary function that transforms the input .json file to a list of dicts.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Output folder that was used during training.\n",
    "    '''\n",
    "\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def plot_train_val_loss(experiment_folder):\n",
    "\n",
    "    '''Plots total and validation losses based on the \"metrics.json\" file generated\n",
    "    in training. The total (training) loss is calculated every 20 iterations (default\n",
    "    step in detectron2 library for loss calculation) and validation loss is calculated \n",
    "    every cfg.TEST.EVAL_PERIOD iterations.\n",
    "\n",
    "    Args:\n",
    "        experiment_folder (str): Output folder that was used during training.\n",
    "    '''\n",
    "\n",
    "    experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "\n",
    "    # Plot total loss.\n",
    "    plt.plot(\n",
    "        [x['iteration'] for x in experiment_metrics if 'total_loss' in x], \n",
    "        [x['total_loss'] for x in experiment_metrics if 'total_loss' in x],\n",
    "        label=\"total_loss\")\n",
    "    \n",
    "    # Plot validation loss curve.\n",
    "    xs = [x['iteration'] for x in experiment_metrics if 'validation_loss' in x]\n",
    "    ys = [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x]\n",
    "    if ys!=[]: plt.plot(xs, ys, c='red', label=\"validation_loss\")\n",
    "    \n",
    "    # Plot validation loss points.\n",
    "    plt.scatter(xs, ys, marker='*', zorder=10, c='black')\n",
    "\n",
    "    # Plot labels.\n",
    "    for x_pos, y_pos in zip(xs, ys):\n",
    "        label = str(round(y_pos,3))\n",
    "        while (len(label)<5): label += '0'\n",
    "        plt.text(x_pos, y_pos+0.05, s=label)\n",
    "    \n",
    "    # Plot properties\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "nXgWtVP0xfs-",
    "outputId": "03293618-9721-47a8-fb49-b8ccdf7bfaca"
   },
   "outputs": [],
   "source": [
    "plot_train_val_loss(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pG_75_rxfs-"
   },
   "outputs": [],
   "source": [
    "def plot_all_losses(experiment_folder, y_log_scale=True, trendlines=True):\n",
    "\n",
    "    '''Plots the 4 different losses (RPN+ROI Heads) of the Faster-RCNN network \n",
    "    based on the \"metrics.json\" file. \n",
    "\n",
    "    Args:\n",
    "        experiment_folder (str): Output folder that was used during training.\n",
    "        y_log_scale (bool): If y-axis is in log scale. \n",
    "        trendlines (bool): Plot the corresponding polynomial trendlines of the losses. \n",
    "    '''\n",
    "\n",
    "    experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    losses = [\"loss_box_reg\", \"loss_cls\", \"loss_rpn_cls\", \"loss_rpn_loc\"]\n",
    "\n",
    "    # Plot loss.\n",
    "    for loss in losses: \n",
    "\n",
    "        # Plot original line.      \n",
    "        xs = [x['iteration'] for x in experiment_metrics if loss in x]\n",
    "        ys = [x[loss] for x in experiment_metrics if loss in x]\n",
    "        order = np.argsort(xs)\n",
    "        p = plt.plot(xs, ys, \"-\", label=loss, linewidth = 1)\n",
    "        \n",
    "        if trendlines:\n",
    "\n",
    "            # Plot polynomial trendline.\n",
    "            z = np.polyfit(np.array(xs)[order.astype(int)], \n",
    "                     np.array(ys)[order.astype(int)], 2)\n",
    "            poly = np.poly1d(z)\n",
    "            plt.plot(np.array(xs)[order.astype(int)], \n",
    "                     poly(np.array(xs)[order.astype(int)]), \n",
    "                     color = p[0].get_color(), linewidth=1.5)\n",
    "\n",
    "    # Plot properties\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    if y_log_scale: plt.yscale('log') \n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "4u_9QUZ80iFd",
    "outputId": "9f0fa251-f24c-4dad-b242-504d0fbfa5a4"
   },
   "outputs": [],
   "source": [
    "plot_all_losses(experiment_folder, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "9b55CxXjXvif",
    "outputId": "2df7d1bd-e6d3-4aab-a7a9-a4c366453ff9"
   },
   "outputs": [],
   "source": [
    "plot_all_losses(experiment_folder, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASCgf1UwxftD"
   },
   "outputs": [],
   "source": [
    "def plot_other_info(experiment_folder):\n",
    "\n",
    "    '''Plots foreground/ backround accuracy, number of foreground and backround\n",
    "    samples as well as the numbers of positive and negative anchors existing in a\n",
    "    batch. \n",
    "\n",
    "    Args:\n",
    "        experiment_folder (str): Output folder that was used during training.\n",
    "    '''\n",
    "\n",
    "    experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "    \n",
    "    mtrics = [\"fast_rcnn/cls_accuracy\", \n",
    "              \"fast_rcnn/false_negative\", \n",
    "              \"fast_rcnn/fg_cls_accuracy\",  \n",
    "              \"roi_head/num_bg_samples\", \n",
    "              \"roi_head/num_fg_samples\", \n",
    "              \"rpn/num_neg_anchors\", \n",
    "              \"rpn/num_pos_anchors\"]\n",
    "\n",
    "    # Plots.\n",
    "    fig, ax1 = plt.subplots(figsize=(16,8))\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    cmaplist = plt.get_cmap('Dark2').colors\n",
    "\n",
    "    for i, mtric in enumerate(mtrics):\n",
    "\n",
    "        xs = [x['iteration'] for x in experiment_metrics if mtric in x]\n",
    "        ys = [x[mtric] for x in experiment_metrics if mtric in x]\n",
    "\n",
    "        # Label properties.\n",
    "        mean = np.array(ys).mean()\n",
    "        label = str(round(mean, 3))\n",
    "        \n",
    "        # Select in which axis you want to plot each metric.\n",
    "        if mtric in mtrics[:3]:\n",
    "            p = ax1.plot(xs, ys, label=mtric, color = cmaplist[i], linewidth=1)\n",
    "            text_color = p[0].get_color()\n",
    "            ax1.text(np.array(xs).mean()+500*i, mean, s=label, color=text_color, zorder=6,  \n",
    "                     size=14, path_effects=[pe.withStroke(linewidth=2, foreground=\"black\")])\n",
    "        else:\n",
    "            p = ax2.plot(xs, ys, label=mtric, color = cmaplist[i], linewidth=1)    \n",
    "            text_color = p[0].get_color()\n",
    "            ax2.text(np.array(xs).mean()+500*i, mean, s=label, color=text_color, zorder=6, \n",
    "                     size=14, path_effects=[pe.withStroke(linewidth=1, foreground=\"black\")])   \n",
    "\n",
    "    # Axis properties\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('FG+BG Accuracy')\n",
    "    ax2.set_ylabel('Number of FG+BG Samples and Positive/Negative Anchors')\n",
    "    ax1.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "rkZ7qTMAxftD",
    "outputId": "cea0372d-5608-40de-f6df-629c36863cdf"
   },
   "outputs": [],
   "source": [
    "plot_other_info(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vXW9fyHxftD"
   },
   "outputs": [],
   "source": [
    "def plot_APs(experiment_folder, labels=True):\n",
    "\n",
    "    '''Plots Average Precision values for different object sizes and values of \n",
    "    the Jaccard index which are calculated in every cfg.TEST.EVAL_PERIOD iterations.\n",
    "\n",
    "    Args:\n",
    "        experiment_folder (str): Output folder that was used during training.\n",
    "        labels (bool): Show AP values. \n",
    "    '''\n",
    "\n",
    "    experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "    plt.figure(figsize=(16,8))\n",
    "\n",
    "    AP_types = [\"bbox/AP\", \"bbox/AP50\", \"bbox/AP75\", \"bbox/APl\", \"bbox/APm\", \"bbox/APs\"]\n",
    "\n",
    "    for AP_type in AP_types:\n",
    "\n",
    "        # Plot AP curve.\n",
    "        xs = [x['iteration'] for x in experiment_metrics if 'validation_loss' in x]\n",
    "        ys = [x[AP_type] for x in experiment_metrics if AP_type in x]\n",
    "        if len(xs)<len(ys):\n",
    "            xs.append(xs[-1]+250)\n",
    "        plt.plot(xs, ys, label=AP_type, linewidth=2)\n",
    "        \n",
    "        # Plot AP points.\n",
    "        plt.scatter(xs, ys, marker='*', zorder=3, c='black', s=10)\n",
    "\n",
    "        # Plot labels.\n",
    "        if labels:\n",
    "            for i, (x_pos, y_pos) in enumerate(zip(xs, ys)):\n",
    "                label = str(round(y_pos,3))\n",
    "                if i%5 ==0: # plot every 5 steps\n",
    "                    while (len(label)<5): label += '0'\n",
    "                    plt.text(x_pos, y_pos+0.005, s=label, fontsize=9, zorder=4,\n",
    "                    path_effects=[pe.withStroke(linewidth=3, foreground=\"white\")])\n",
    "    \n",
    "    # Plot properties\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('AP')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "6W_an0eOxftE",
    "outputId": "936d36fb-9371-44ff-f2ca-6b16ed6c415d"
   },
   "outputs": [],
   "source": [
    "plot_APs(experiment_folder, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "ohhpyfXK1_90",
    "outputId": "b88787a7-c860-4f80-992e-0a29710c6228"
   },
   "outputs": [],
   "source": [
    "plot_APs(experiment_folder, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6Yg7YXxxftE"
   },
   "source": [
    "# **Download results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAJPVyXR1Wip"
   },
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4PL_YU7xftE"
   },
   "outputs": [],
   "source": [
    "# Output training files:\n",
    "directory_path = cfg.OUTPUT_DIR\n",
    "%cd $directory_path\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2H5Wo7qxftF"
   },
   "outputs": [],
   "source": [
    "# Zip the above folder to ex.zip\n",
    "%cd /kaggle/working\n",
    "!zip -r ex.zip $cfg.OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIc9Iu8ExftF"
   },
   "source": [
    "## Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPrISSKn4hKL"
   },
   "outputs": [],
   "source": [
    "# Zip output folder\n",
    "!zip -r experiments.zip $cfg.OUTPUT_DIR\n",
    "\n",
    "# Download the results\n",
    "from google.colab import files\n",
    "files.download('experiments.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kblA1IyFvWbT"
   },
   "source": [
    "# Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9tECBQCvMv3",
    "outputId": "8e50e0be-cc03-415c-b4a0-eeb31debc32f"
   },
   "outputs": [],
   "source": [
    "# Load model's configuration\n",
    "cfg_path = '/content/drive/MyDrive/FINAL_MODELS/FRCNN_normal/configuration_07_02_2022__12_59.yaml' #'./configuration_07_02_2022__12_59.yaml'\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "# Change some configuration properties\n",
    "cfg.MODEL.WEIGHTS = '/content/drive/MyDrive/FINAL_MODELS/FRCNN_normal/model_final.pth' #cfg.OUTPUT_DIR + '/' + 'model_final.pth' # loads trained weights\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 3000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 3000\n",
    "cfg.DATASETS.TRAIN = (dataset_names[train_id],) \n",
    "cfg.DATASETS.TEST = (dataset_names[val_id],) \n",
    "cfg.OUTPUT_DIR = '/content' #define the directory that contains the metrics.json file generated during training.\n",
    "\n",
    "# Build the model from configuration file and MyTrainer():\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=True)\n",
    "model = trainer.model # has the same result as detectron2.modeling.build_model but it does not load weights! We must use before cfg.MODEL.WEIGHTS = ...\n",
    "\n",
    "# Buld evaluator and data loader. \n",
    "evaluator = COCOEvaluator(dataset_names[test_id], output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, dataset_names[test_id], mapper_wo)\n",
    "\n",
    "# Evaluate:\n",
    "print(inference_on_dataset(model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH72BFOSY8-N"
   },
   "source": [
    "# Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNDKyoU72YrZ"
   },
   "source": [
    "## Inference on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya5nEuMELeq8"
   },
   "outputs": [],
   "source": [
    "# Load and change some configuration parameters:\n",
    "cfg_path = '/content/drive/MyDrive/FINAL_MODELS/FRCNN_normal/configuration_07_02_2022__12_59.yaml' #'./configuration_07_02_2022__12_59.yaml'\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "cfg.defrost()\n",
    "cfg.MODEL.WEIGHTS = '/content/drive/MyDrive/FINAL_MODELS/FRCNN_normal/model_final.pth'          #cfg.OUTPUT_DIR + '/' + 'model_final.pth'\n",
    "cfg.DATASETS.TRAIN = (dataset_names[train_id],) \n",
    "cfg.DATASETS.TEST = (dataset_names[val_id],)   \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set a custom testing threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBW2Un_oKUdS"
   },
   "outputs": [],
   "source": [
    "def show_results(noi_2_display, dataset_dicts, dataset_metadata, cfg):\n",
    "\n",
    "    \"\"\"Shows a given number of images (>=2) with their predictions/annotations.\n",
    "\n",
    "    Args:\n",
    "        noi_2_display (int): Number of images to display.\n",
    "        dataset_dicts (list): Input dataset's dicts.\n",
    "        dataset_metadata (detectron2.data.Catalog.Metadata): Input dataset's metadata.\n",
    "        cfg (detectron2.config.config.CfgNode): Model's configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    noi_2_display = int(noi_2_display)\n",
    "    if noi_2_display%2 !=0: noi_2_display += 1 \n",
    "\n",
    "    # Number of images on each row/column\n",
    "    imgs_4_display_in_side = 2 \n",
    "    if noi_2_display%5==0: imgs_4_display_in_side = 5 \n",
    "    if (noi_2_display>=8 and imgs_4_display_in_side!=5): imgs_4_display_in_side = 4 \n",
    "    image_rows = int(noi_2_display/imgs_4_display_in_side)\n",
    "\n",
    "    predictor = DefaultPredictor(cfg) \n",
    "\n",
    "    for i,d in enumerate(random.sample(dataset_dicts, noi_2_display)):    \n",
    "        temp_val_image = cv2.imread(d[\"file_name\"]) # read a random image\n",
    "\n",
    "        # Predict amd draw instances in the above random image.\n",
    "        outputs = predictor(temp_val_image)  \n",
    "        v = Visualizer(temp_val_image, metadata=dataset_metadata, scale=0.8, instance_mode=ColorMode.SEGMENTATION)\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        \n",
    "        # Extract input image's shape.\n",
    "        image_shape = out.get_image().shape\n",
    "        \n",
    "        # Initialization of ndarrays.\n",
    "        if i==0:\n",
    "            stacked_images = np.zeros(shape=out.get_image().shape+(noi_2_display,))\n",
    "            rows = np.zeros(shape=(image_shape[0], # height\n",
    "                                    image_shape[1]*imgs_4_display_in_side, # width\n",
    "                                    image_shape[2], # number of channels \n",
    "                                    noi_2_display)) \n",
    "        \n",
    "        # Put text and stack the above annotated image into an ndarray.  \n",
    "        annotated_image = out.get_image()  \n",
    "\n",
    "        cv2.putText(annotated_image, 'Image ID: ' +  str(d['image_id']),\n",
    "                    (int(0.75*image_shape[1]), int(0.95*image_shape[0])), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255, 0), 1) \n",
    "        \n",
    "        stacked_images[:,:,:,i] = annotated_image\n",
    "    \n",
    "    # Create one row at a time.\n",
    "    for i in range(image_rows):\n",
    "        rows[:,:,:,i] = np.hstack([stacked_images[:,:,:,hor_number]\n",
    "                                  for hor_number in range(i*imgs_4_display_in_side, \n",
    "                                                          i*imgs_4_display_in_side+\n",
    "                                                          imgs_4_display_in_side)])\n",
    "    \n",
    "    # Stich all rows together.\n",
    "    collage = np.vstack([rows[:,:,:,i] for i in range(image_rows)])\n",
    "    \n",
    "    # Show results.\n",
    "    plt.figure(figsize=(30, 15))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(np.uint8(collage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kDG1xR8FKiBM",
    "outputId": "2b95dc6e-2727-4ffc-ab38-b97fbc38fdad"
   },
   "outputs": [],
   "source": [
    "noi_2_display = 4 #number of images for displaying.\n",
    "for i in range(2): show_results(noi_2_display, dicts_test, metadata_test, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcfwO8cMjZnT"
   },
   "source": [
    "## Inference on video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKOxyBAhjgnt"
   },
   "outputs": [],
   "source": [
    "#Clone detectron2's repository from github.\n",
    "!git clone https://github.com/facebookresearch/detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZvjdoGlH5o6"
   },
   "outputs": [],
   "source": [
    "# Load model's configuration:\n",
    "cfg_path = '/content/drive/MyDrive/FINAL_MODELS/FRCNN_normal/configuration_07_02_2022__12_59.yaml' \n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "cfg.defrost()\n",
    "cfg.MODEL.WEIGHTS = '/content/drive/MyDrive/FINAL_MODELS/FRCNN_normal/model_final.pth'       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKVKDDXRjpS5"
   },
   "outputs": [],
   "source": [
    "# Run frame-by-frame inference demo on this video with the \"demo.py\" tool provided in the detectron2's repository.\n",
    "%run detectron2/demo/demo.py \\\n",
    "  --config-file $cfg_path \\\n",
    "  --video-input Base_vid.wmv --confidence-threshold 0.25 --output video-output.mkv \\\n",
    "  --opts MODEL.WEIGHTS $cfg.MODEL.WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2nj58udjt36"
   },
   "outputs": [],
   "source": [
    "# Download the results\n",
    "from google.colab import files\n",
    "files.download('video-output.mkv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Faster-RCNN-Detectron2-Normal-Bounding-Boxes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
